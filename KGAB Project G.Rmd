---
title: "KGAB Project G"
author: "Kemardo Tyrell, Giovanna Morara, Benjamin Leslie, and Alex Arhin"
date: March 14, 2023
output: html_notebook
---

*This project will require you to collaborate with some of your fellow students to complete a statistical analysis and report. Here are some of the tasks that you must accomplish.*

- Create a public GitHub repository with an RStudio project file (team leader)
- Put a description of the project in the README.md file (team leader)
- Email team members and Mike the repository address (team leader)
- Write a short introduction
- Provide a graphical display and descriptive statistics to compare groups
- Conduct an analysis of variance (ANOVA) for omnibus inference
- Check the conditions necessary for valid ANOVA inference
- Conduct a bootstrap for omnibus inference without needing conditions
- Construct Tukey pairwise confidence intervals (if appropriate)
- Write a conclusions section
- Assemble a complete knittable report (team leader)

*If you write any functions, put these in a **Functions** folder. Put data in a **Data** folder. These should be subfolders in your main project folder. Your finished notebook should be neat and organized. Save this notebook with your team name and **Project G** in the file name, rather than the report title. Leave the instructions in place and begin your report after the last horizontal line below. All team members will receive the same score for this project. (40 points possible)*

***

The *HSB2 Data* includes variables collected on a random sample of high school seniors. Conduct an analysis to compare performance on the test variable assigned to your team for the three socioeconomic groups. Include a graphical display and descriptive statistics. Also include an omnibus analysis that assumes valid conditions for parametric inference (ANOVA) as well as an omnibus analysis that does not assume these conditions (bootstrap). Construct pairwise Tukey confidence intervals, if appropriate.

***
```{r}
# Clear the environment
rm(list = ls())

# Load the libraries
library(here)
library(dplyr)

# Load data
data <- read.csv(here("Data", "hsb2.csv"))
the_data <- read.csv(here("Data", "hsb2.csv"))
```


## Introduction



***

## Analysis of Variance

#### Describing Data


```{r}
# ses ordinal  (1=low 2=middle 3=high)  
data$ses <- factor(data$ses,
                   labels = c("Low",
                              "Middle",
                              "High"))

# Obtain boxplots.

boxplot(data$science ~ data$ses,
        ylim = c(0, 100),
        xlab = "Socioeconomic Status",
        ylab = "Science Scores",
        main = "Scores depending on SES")

```

#### Obtain descriptive statistics.

```{r echo=FALSE}
tapply(data$science, data$ses, summary)
tapply(data$science, data$ses, sd)
```


#### Check the conditions necessary for valid ANOVA inference

###### Groups are independent (research design)

This study is observational so individuals are classified to one SES.

###### Units within groups are independent to each other (research design)

The students are randomly selected. It could still be possible that near
relatives are from one family (therefore non independent).

###### Now let's check the conditions for inference (statistic)

```{r echo=FALSE}
model <- lm(data$science ~ data$ses)
std_residual <- rstandard(model)
qqnorm(std_residual)
qqline(std_residual)
```

The sampling distribution is normally distributed, although there is 
a deviation for higher levels. However, for each group we have more
than 30 participants and the deviations is not harsh. Therefore, the 
condition is met.

```{r echo=FALSE}
table(data$ses)
```

###### Residual withint groups have the same variation

Looking at the variance of residuals within the categories. 

```{r echo=FALSE}
plot(model$fitted.values, model$residuals,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Plot of Residuals vs. Fit")

abline(0,0)
```

There is some skewness in the in the High SES. But the spread around the 
means homogeneous for the most cases.

###### Sampling distribution of means are normal distributions

We do not have a skewed distribution, we also have more than 30
participant for each group so for the Central Limit Theorem, so we have
a normal sampling distribution. 

---------------------------------------------------------------------------

#### Conduct an analysis of variance (ANOVA) for omnibus inference

We will use function aov to get the analysis of variance

```{r echo=FALSE}
aov.model <- aov(data$science ~ data$ses)
aov.model
summary(aov.model)
```
Choosing a 95% certanty level, we can reject the null hypothesis: there is
some difference between one or more of the groups.

#### Analysis of pairs of mean.

This section is to construct confidence intervals for each pair of 
conditions to understand the differences between the groups.

```{r echo=FALSE}
## Checking dummies variables
# model <- lm(data$science ~ data$ses)
# summary(model)
# anova(model)
```


```{r echo=FALSE}
ttest1 <- t.test(science ~ ses,
           data = data,
           subset = (ses == "Low" | ses == "Middle"))
ttest1

ttest2 <- t.test(science ~ ses,
           data = data,
           subset = (ses == "Middle" | ses == "High"))
ttest2

ttest3 <- t.test(science ~ ses,
           data = data,
           subset = (ses == "Low" | ses == "High"))
ttest3
```

There seems to be differences for each pair of means. Specifically, science scores for high ses students are higher then both middle and low ses. The 95% confidence interval for the mean difference was found to be `r round(ttest2$conf.int[1],2)` < $mu$ < `r round(ttest2$conf.int[2],2)` when compared to middle SES students and `r round(ttest3$conf.int[1],2)` < $mu$ < `r round(ttest3$conf.int[2],2)` when compared to low SES students. At the same time, middle ses students have higher science score than low ses students. The 95% confidence interval for the mean difference was found to be `r round(ttest1$conf.int[1],2)` < $mu$ < `r round(ttest1$conf.int[2],2)

***

## Inference and Confidence Interval
```{r echo=FALSE}
 # Initialize objects
 mean_vector <- NULL
 n <- length(the_data$science)
```

```{r echo=FALSE}
 # Take multiple samples (with replacement) and construct sampling distribution 
 # of the mean
 
 # Setting a seed for reproducibility
 set.seed(123)

 for (i in 1:10000) {
   the_sample <- sample(the_data$science, n, replace = TRUE)
   mean_vector <- c(mean_vector, mean(the_sample))
 }

 # Sort the vector and cut off 2.5% on each end
 mean_vector <- sort(mean_vector) 
 
 # Using the quantile function so that the bounds are not
 # dependent on the number of bootstrap samples generated
 boot_int <- quantile(mean_vector, c(0.025, 0.975))
 boot_int
   
```

The code above uses bootstrapping to construct a sampling distribution of the mean of the "science" column in the data frame or list "the_data". The code generates 10,000 bootstrap samples of size "n" with replacement from the "science" column and computes the mean of each sample. The resulting means are stored in the "mean_vector" variable, which contains the distribution of the means of the bootstrap samples. The "mean_vector" is then sorted in ascending order and the lower and upper bounds of the 95% bootstrap confidence interval are extracted using the quantile() function. This way, the bounds are not dependent on the number of bootstrap samples generated. The resulting confidence interval was found to be `r round(boot_int[1],2)` < $mu$ < `r round(boot_int[2],2)`

```{r echo=FALSE}
# Check for equal sample sizes
low <- subset(the_data, the_data$ses == 1)
count(low)
middle <- subset(the_data, the_data$ses == 2)
count(middle)
high <- subset(the_data, the_data$ses == 3)
count(high)

# Check for equal variance
low <- low[,10]
sd(low)
middle <- middle[,10]
sd(middle)
high <- high[,10]
sd(high)
```


There is no need to construct pairwise key confidence intervals since this is typically used in the context of comparing multiple groups, often through one-way ANOVA or a similar analysis of variance method. Since this report is dealing with a single column of data (science), it does not make sense to construct such intervals since there is only one group to compare. Instead, when dealing with a single column of data, it may not make sense to construct Tukey pairwise confidence intervals since there is only one group to compare. Instead, one can use bootstrapping or other re-sampling methods to estimate the uncertainty around the mean or other summary statistic of the data, and to construct confidence intervals for that statistic. Hence, bootstrapping was done earlier.

***

## Conclusions
An analysis of variance was conducted to compare HSB2 science performances for three socioeconomic groups. The data set was found to satisfy the criteria for conducting a valid ANOVA as the groups were independent, the units within groups were independent, the data is normally distributed, and the distribution is not skewed. Using the ANOVA, we can say with a 95% level of confidence that there is not a statistically significant difference between two or more of the groups. T tests were then used to compare the mean science scores for each SES level. These tests found high SES students obtained higher scores than middle or low SES students. The results also found that middle SES students obtained higher scores than low SES students.

